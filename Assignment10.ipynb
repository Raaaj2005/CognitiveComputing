{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a472d-f2c6-4b62-a9a2-c5ead0856dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "paragraph = \"\"\"\n",
    "Technology has transformed the way we live, work, and connect with others. From smartphones that keep us constantly linked to the internet, to artificial intelligence reshaping industries, the impact is profound. I enjoy exploring new gadgets and learning about the latest advancements in computing. Innovation in technology not only solves real-world problems but also inspires creativity. As we continue to push the boundaries of what machines can do, it’s fascinating to think about where we’re headed next.\n",
    "\"\"\"\n",
    "\n",
    "lowercase_text = paragraph.lower()\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', lowercase_text)\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "word_tokens_nltk = word_tokenize(clean_text)\n",
    "word_tokens_split = clean_text.split()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in word_tokens_nltk if word not in stop_words]\n",
    "\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Python split():\", word_tokens_split)\n",
    "print(\"NLTK word_tokenize():\", word_tokens_nltk)\n",
    "print(\"Filtered Tokens (no stopwords):\", filtered_tokens)\n",
    "print(\"Most Common Words:\", freq_dist.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "freq_dist.plot(20, title=\"Top Word Frequencies (Excluding Stopwords)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a6a53-348d-4629-860f-5c63e090af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "paragraph = \"\"\"\n",
    "Technology has transformed the way we live, work, and connect with others. From smartphones that keep us constantly linked to the internet, to artificial intelligence reshaping industries, the impact is profound. I enjoy exploring new gadgets and learning about the latest advancements in computing. Innovation in technology not only solves real-world problems but also inspires creativity. As we continue to push the boundaries of what machines can do, it’s fascinating to think about where we’re headed next.\n",
    "\"\"\"\n",
    "\n",
    "words = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Original Words:\", filtered_words)\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "print(\"\\nLemmatized Words:\", lemmatized_words)\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f96389-cbcc-4189-8d49-56c548122a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"The stock market saw a significant decline today as inflation worries grow.\",\n",
    "    \"The new smartphone model features cutting-edge technology and sleek design.\",\n",
    "    \"The book was a thrilling adventure with twists and turns at every chapter.\"\n",
    "]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(texts)\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag of Words Representation (CountVectorizer):\")\n",
    "print(count_matrix.toarray())\n",
    "print(\"Feature Names:\", count_features)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"TF-IDF Feature Names:\", tfidf_features)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nTop 3 Keywords for Text {i+1}:\")\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_3_indices = np.argsort(tfidf_scores)[::-1][:3]\n",
    "    top_3_keywords = [(tfidf_features[index], tfidf_scores[index]) for index in top_3_indices]\n",
    "    for keyword, score in top_3_keywords:\n",
    "        print(f\"{keyword}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e19af2-0376-4f11-af62-b7a9d04f6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. AI is transforming industries such as healthcare, finance, and robotics, offering advancements in automation, data processing, and decision-making.\",\n",
    "    \"Blockchain is a decentralized technology that records transactions across many computers securely and transparently. It enables cryptocurrencies like Bitcoin, and its applications extend to secure online voting, supply chain management, and digital contracts.\"\n",
    "]\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokens_1 = preprocess_and_tokenize(texts[0])\n",
    "tokens_2 = preprocess_and_tokenize(texts[1])\n",
    "\n",
    "set_1 = set(tokens_1)\n",
    "set_2 = set(tokens_2)\n",
    "jaccard_similarity = len(set_1.intersection(set_2)) / len(set_1.union(set_2))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity}\")\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "\n",
    "if jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb120b-dd40-4745-a259-13773d820638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reviews = [\n",
    "    \"This product is amazing! It has exceeded all my expectations and works flawlessly.\",\n",
    "    \"I am really disappointed with this service. It didn't meet any of my needs and was a waste of money.\",\n",
    "    \"The quality of the product is decent, but it's nothing special. It's just okay.\"\n",
    "]\n",
    "\n",
    "def analyze_sentiment_textblob(review):\n",
    "    blob = TextBlob(review)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    return polarity, subjectivity\n",
    "\n",
    "def analyze_sentiment_vader(review):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    return sentiment['compound']\n",
    "\n",
    "textblob_results = [analyze_sentiment_textblob(review) for review in reviews]\n",
    "vader_results = [analyze_sentiment_vader(review) for review in reviews]\n",
    "\n",
    "def classify_review(polarity, method=\"textblob\"):\n",
    "    if method == \"textblob\":\n",
    "        if polarity > 0:\n",
    "            return \"Positive\"\n",
    "        elif polarity < 0:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "    else:\n",
    "        if polarity > 0.05:\n",
    "            return \"Positive\"\n",
    "        elif polarity < -0.05:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "\n",
    "textblob_classifications = [classify_review(result[0], \"textblob\") for result in textblob_results]\n",
    "vader_classifications = [classify_review(result, \"vader\") for result in vader_results]\n",
    "\n",
    "positive_reviews = [review for i, review in enumerate(reviews) if textblob_classifications[i] == \"Positive\"]\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(positive_reviews))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"TextBlob Results:\", textblob_classifications)\n",
    "print(\"VADER Results:\", vader_classifications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86a2b6-df6c-4648-b1f4-af952697ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence is transforming many industries, providing innovative solutions that are reshaping how businesses operate. AI technologies, such as machine learning, are becoming increasingly advanced and are being applied to a wide range of fields, from healthcare to finance. The impact of AI is not limited to just improving efficiency and accuracy, but also to opening up new opportunities for creativity and growth. As AI continues to evolve, it will likely play an even more prominent role in shaping the future.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=50, verbose=1)\n",
    "\n",
    "seed_text = \"artificial\"\n",
    "for _ in range(3):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = tokenizer.index_word[predicted[0]]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
